{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcxDYcbjbmb718ANf4c04N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ijjoe/Bul4jo/blob/main/240419_%EC%9C%84%EC%8A%A4%ED%8D%BC%ED%85%8C%EC%8A%A4%ED%8A%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9laS1r5VvBA",
        "outputId": "5b44874e-1a6d-4c5c-8a9d-9b56c3f36e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.4)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->openai-whisper)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->openai-whisper)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801358 sha256=5c5de5a5a352d8132be858f08e5c02b5cd33ff666fc33503f1285d6dde68ec8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 openai-whisper-20231117 tiktoken-0.6.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update && sudo apt install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMzhvbC1VxJX",
        "outputId": "b6833572-7554-47b7-afdd-88f77458d1e0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [1 InRelease 3,626 B/3,626 B 100\u001b[0m\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [811 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,369 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,307 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,019 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.1 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,077 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,739 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,228 kB]\n",
            "Fetched 11.8 MB in 9s (1,348 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper"
      ],
      "metadata": {
        "id": "vZj3vV9kW-Py"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# load audio and pad/trim it to fit 30 seconds\n",
        "audio = whisper.load_audio(\"01_01_215115_220729_0028_VN.wav\")\n",
        "audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "# make log-Mel spectrogram and move to the same device as the model\n",
        "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "# detect the spoken language\n",
        "_, probs = model.detect_language(mel)\n",
        "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
        "\n",
        "# decode the audio\n",
        "options = whisper.DecodingOptions()\n",
        "result = whisper.decode(model, mel, options)\n",
        "\n",
        "# print the recognized text\n",
        "print(result.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhySGITKXAYD",
        "outputId": "ed992706-4d7f-46f0-f5e6-ac71dcd34b28"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language: ko\n",
            "당신이 숫자도 색깔로 볼 수 있다고 하셨잖아요. 그래서 감사하게도 제가 좋아하는 숫자인 명과 부의 색깔을 알려주셨잖아요. 그런데 당신이 좋아하는 숫자에 대해서는 얘기를 못한 것 같아요. 제일 좋아하는 숫자는 뭐예요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!whisper \"01_01_215115_220729_0028_VN.wav\" --language Korean --model base"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw-lOR16WM-i",
        "outputId": "567f5b6c-be4f-4e3e-d6a5-f83a9018286e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "[00:00.000 --> 00:06.960]  당신이 숫자도 색깔로 볼 수 있다고 하셨잖아요.\n",
            "[00:06.960 --> 00:15.880]  그래서 감사하게도 제가 좋아하는 숫자인 명과 그의 색깔을 알려주셨잖아요.\n",
            "[00:15.880 --> 00:21.040]  그런데 당신이 좋아하는 숫자에 대해서는 얘기를 못한 것 같아요.\n",
            "[00:21.040 --> 00:25.080]  제일 좋아하는 숫자는 뭐예요?\n",
            "[00:25.080 --> 00:30.280]  제가 제일 좋아하는 숫자는 43이라는 숫자예요.\n",
            "[00:30.280 --> 00:34.320]  혹시 이유가 궁금하지 않으신가요?\n",
            "[00:34.320 --> 00:52.600]  보통 저처럼 영이나 후 이렇게 한 자리로 얘기를 하시는 분들이 많을 텐데 43이라는 어중관하다면 어중관할 수 있고 깔끔하게 끝나지도 않은 그런 숫자를 어떤 일으로 좋아하시는지 궁금해지네요.\n",
            "[00:52.600 --> 00:55.880]  일단 정말 다양한 이유가 있어요.\n",
            "[00:55.880 --> 01:04.080]  제가 아까 말씀드렸듯이 저는 그 자에서 색깔을 볼 수 있는 공감각자라고 말씀 드렸잖아요.\n",
            "[01:04.080 --> 01:14.760]  그런데 제가 43, 4와 3에서 느끼는 색깔 때문에 그 숫자를 더 좋아하게 됐어요.\n",
            "[01:14.760 --> 01:23.560]  제가 느끼기에 숫자 사이에서 연부농빛을 보이고 딱이 우유채 그런 연부농색이 보여요.\n",
            "[01:23.560 --> 01:26.840]  그리고 3에서는 레몬색이 보이거든요.\n",
            "[01:26.840 --> 01:33.480]  그 연부농빛과 레몬 빛의 조합이 정말 예쁘더라고요.\n",
            "[01:33.480 --> 01:47.800]  43이라는 숫자를 보고 있으면 연부농빛과 레몬빛이 예쁘게 조화가 되어서 정말 아름다운 빛가를 만들어내고 있는 게 제 눈으로 보이더라고요.\n",
            "[01:47.800 --> 01:56.800]  그래서 공감각자랑 43에서 보이는 색깔적인 특성으로 인해서 그 숫자를 더 좋아하게 됐죠.\n",
            "[01:56.800 --> 01:59.920]  또 궁금하신 게 있나요?\n",
            "[01:59.920 --> 02:03.280]  파스텔 톤을 좋아한다고 하셨잖아요.\n",
            "[02:03.280 --> 02:12.240]  이 연부농빛과 레몬색이 적절히 합쳐지면 예쁜 파스텔 톤처럼 나올 것 같은데\n",
            "[02:12.240 --> 02:17.040]  그래서 43이라는 숫자가 더 좋아지신 건가요?\n",
            "[02:17.040 --> 02:18.240]  맞습니다.\n",
            "[02:18.240 --> 02:28.040]  제가 아무래도 파스텔 톤의 색깔들을 좋아하다 보니까 그런 색깔들에서 더욱 더 매력을 느끼게 된 것 같아요.\n",
            "[02:28.040 --> 02:35.120]  그리고 그런 이유를 통해서 43이라는 숫자를 굉장히 좋아하고 있었는데요.\n",
            "[02:35.120 --> 02:38.680]  또 다른 이유가 생겼어요.\n",
            "[02:38.680 --> 02:42.960]  제가 정말 정말 좋아하는 연예인 분이 생겼거든요.\n",
            "[02:42.960 --> 02:48.360]  그래서 그 아티스트 분 덕지를 하고 팬으로서 팬미팅도 하고\n",
            "[02:48.360 --> 02:53.640]  펜사마다 쫓아다닐 정도로 그렇게 열려란 펜활동을 하고 있다가\n",
            "[02:53.640 --> 02:57.200]  문득 깨달은 게 있어요.\n",
            "[02:57.200 --> 03:01.840]  그 분의 생일이 바로 4월 3일이었던 거예요.\n",
            "[03:01.840 --> 03:07.240]  안그래도 43이라는 숫자를 좋아하고 있었잖아요.\n",
            "[03:07.240 --> 03:16.160]  게다가 제 인생을 바꿔주실 정도고 정말 저에게 큰 영향을 끼치신 아티스트 분의 4월 3일이\n",
            "[03:16.160 --> 03:21.120]  생일이라고 하니까 정말 놀랍더라고요.\n",
            "[03:21.120 --> 03:27.680]  그때부터 43이라는 숫자를 더욱 더 의미 있게 좋아하게 됐어요.\n",
            "[03:27.680 --> 03:37.360]  마침 그 분도 본인의 생일 덕분에 좋아하는 숫자가 43이라고 말씀을 하시더라고요.\n",
            "[03:37.360 --> 03:40.360]  그리고 마지막 이유가 또 하나 있는데요.\n",
            "[03:40.360 --> 03:47.000]  제가 애니어브랜드 분사를 했는데 그 결과가 4W3이 나온 거예요.\n",
            "[03:47.000 --> 03:49.000]  그래서 너무 신기했죠.\n",
            "[03:49.000 --> 03:55.560]  나는 어떻게 애니어브랜드 분사까지 그 숫자가 나올까 하고 말이에요.\n",
            "[03:55.560 --> 04:08.920]  그러니까 43처럼 보이는 그런 결과가 나올 수 있다는 것 자체가 정말 44은 나의 운명이구나 하는 생각이 들어서\n",
            "[04:08.920 --> 04:12.120]  더욱 더 좋아하게 됐죠.\n",
            "[04:12.120 --> 04:20.360]  좋아하는 이유랑 또 이 좋아하는 숫자가 운명적으로 잡고 주변을 기울 기울거리는데\n",
            "[04:20.360 --> 04:23.720]  그 숫자를 정말 좋아할만 하네요.\n",
            "[04:23.720 --> 04:30.920]  이 정도면 정말 당신한테는 운명의 숫장인 것 같다라는 그런 생각이 들어요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TxzfUDI6ZQ3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}